import type { Express, Request, Response, NextFunction } from "express";
import express from "express";
import { createServer, type Server } from "http";
import { storage } from "./storage";
import { authenticateUser, createUser, validateUsername, validatePassword } from "./auth";
import type { AuthenticatedRequest } from "./types";
import { 
  createNotionClient, 
  extractPageIdFromUrl, 
  createJournalDatabase, 
  addJournalEntryToNotion,
  getNotionDatabases 
} from "./notion";
import { syncJournalEntryToNotion, syncAllUserEntriesToNotion, cleanupNotionEntriesForMerge } from "./notion-sync";
import { splitEntriesByDate, formatDateForLog } from "./date-parser";

import { 
  insertJournalEntrySchema, 
  insertThemeSchema, 
  insertTagSchema,
  insertEntryTagSchema,
  insertSentimentAnalysisSchema
} from "@shared/schema";
import { analyzeJournalEntry, analyzeSentiment, extractTextFromImage } from "./openai";
import { retryFailedEntries } from "./retry-processing";
import multer from "multer";
import path from "path";
import fs from "fs/promises";
import { exec } from "child_process";
import { promisify } from "util";

const execAsync = promisify(exec);

// In-memory batch tracking
const activeBatches = new Map<string, number[]>();

// Helper function to process AI analysis results
async function processEntryAnalysis(entryId: number, analysisResult: any, sentimentResult: any) {
  // Add themes
  for (const theme of analysisResult.themes) {
    await storage.createTheme({
      entryId: entryId,
      title: theme.title,
      description: theme.description,
      confidence: Math.round(Number(theme.confidence))
    });
  }

  // Add tags
  for (const tagName of analysisResult.tags) {
    const tag = await storage.getOrCreateTag(tagName);
    await storage.addTagToEntry({
      entryId: entryId,
      tagId: tag.id,
      confidence: 80,
      isAutoGenerated: true
    });
  }

  // Add sentiment analysis
  await storage.createSentimentAnalysis({
    entryId: entryId,
    positiveScore: sentimentResult.positive,
    neutralScore: sentimentResult.neutral,
    concernScore: sentimentResult.concern,
    overallSentiment: sentimentResult.overall
  });
}

// Sequential bulk processing function
async function processBulkEntriesSequentially(entries: any[], userId: number, batchId: string) {
  console.log(`Starting sequential processing of ${entries.length} entries for user ${userId}, batch ${batchId}`);
  
  for (let i = 0; i < entries.length; i++) {
    const entry = entries[i];
    console.log(`Processing entry ${i + 1}/${entries.length}: ${entry.id}`);
    
    try {
      const ocrResult = await extractTextFromImage(entry.filePath);
      
      if (ocrResult.text && ocrResult.text.trim().length > 0) {
        console.log(`OCR completed for entry ${entry.id}, checking for transcript duplicates...`);
        
        // Check for transcript duplicates
        const { computeTranscriptHash } = await import('./image-hash');
        const transcriptHash = computeTranscriptHash(ocrResult.text);
        
        const existingTranscriptEntry = await storage.getJournalEntryByTranscriptHash(transcriptHash);
        if (existingTranscriptEntry) {
          console.log(`Duplicate transcript detected for entry ${entry.id} (matches entry ${existingTranscriptEntry.id}), marking as failed`);
          await storage.updateJournalEntry(entry.id, {
            processingStatus: "failed",
            transcribedText: `Duplicate content detected (matches entry ${existingTranscriptEntry.id})`
          });
          continue;
        }
        
        console.log(`No transcript duplicates found for entry ${entry.id}, checking for multiple dates...`);
        
        // Check for multiple dates and split if needed
        const splitEntries = splitEntriesByDate(ocrResult.text);
        console.log(`Found ${splitEntries.length} date-based entries in OCR text`);
        
        if (splitEntries.length > 1) {
          // Multiple entries detected - process each separately
          console.log(`Splitting entry ${entry.id} into ${splitEntries.length} separate entries`);
          
          // Update the original entry with the first split
          const firstEntry = splitEntries[0];
          console.log(`Processing first split entry with date: ${formatDateForLog(firstEntry.date)}`);
          
          const firstAnalysis = await analyzeJournalEntry(firstEntry.content);
          const firstSentiment = await analyzeSentiment(firstEntry.content);
          
          await storage.updateJournalEntry(entry.id, {
            title: firstAnalysis.title || `Journal Entry - ${formatDateForLog(firstEntry.date)}`,
            transcribedText: firstEntry.content,
            ocrConfidence: ocrResult.confidence,
            processingStatus: "completed",
            transcriptHash
          } as any);
          
          // Process themes, tags, and sentiment for first entry
          await processEntryAnalysis(entry.id, firstAnalysis, firstSentiment);
          
          // Create new entries for remaining splits
          for (let j = 1; j < splitEntries.length; j++) {
            const splitEntry = splitEntries[j];
            console.log(`Creating new entry for split ${j + 1} with date: ${formatDateForLog(splitEntry.date)}`);
            
            // Compute hash for each split content
            const splitTranscriptHash = computeTranscriptHash(splitEntry.content);
            
            const newEntry = await storage.createJournalEntry({
              userId: entry.userId,
              title: `Journal Entry - ${formatDateForLog(splitEntry.date)}`,
              originalImageUrl: entry.originalImageUrl, // Reuse same image
              transcribedText: splitEntry.content,
              ocrConfidence: ocrResult.confidence,
              processingStatus: "completed",
              transcriptHash: splitTranscriptHash
            } as any);
            
            const splitAnalysis = await analyzeJournalEntry(splitEntry.content);
            const splitSentiment = await analyzeSentiment(splitEntry.content);
            
            // Update title with AI-generated one
            await storage.updateJournalEntry(newEntry.id, {
              title: splitAnalysis.title || newEntry.title
            });
            
            // Process themes, tags, and sentiment for split entry
            await processEntryAnalysis(newEntry.id, splitAnalysis, splitSentiment);
            
            // Sync split entry to Notion
            try {
              const completeNewEntry = await storage.getJournalEntry(newEntry.id);
              if (completeNewEntry) {
                const syncSuccess = await syncJournalEntryToNotion(completeNewEntry);
                if (syncSuccess) {
                  console.log(`Split entry ${newEntry.id} synced to Notion successfully`);
                }
              }
            } catch (notionError) {
              console.error(`Notion sync failed for split entry ${newEntry.id}:`, notionError);
            }
          }
          
          // Sync original entry to Notion
          try {
            const completeEntry = await storage.getJournalEntry(entry.id);
            if (completeEntry) {
              const syncSuccess = await syncJournalEntryToNotion(completeEntry);
              if (syncSuccess) {
                console.log(`Original entry ${entry.id} synced to Notion successfully`);
              }
            }
          } catch (notionError) {
            console.error(`Notion sync failed for entry ${entry.id}:`, notionError);
          }
          
          console.log(`Entry ${entry.id} split into ${splitEntries.length} entries successfully`);
        } else {
          // Single entry - process normally
          console.log(`Processing single entry ${entry.id}`);
          const singleEntry = splitEntries[0];
          
          const analysisResult = await analyzeJournalEntry(singleEntry.content);
          const sentimentResult = await analyzeSentiment(singleEntry.content);

          await storage.updateJournalEntry(entry.id, {
            title: analysisResult.title || entry.title,
            transcribedText: singleEntry.content,
            ocrConfidence: ocrResult.confidence,
            processingStatus: "completed",
            transcriptHash
          } as any);

          // Process themes, tags, and sentiment
          await processEntryAnalysis(entry.id, analysisResult, sentimentResult);

          // Sync to Notion if integration is enabled
          try {
            const completeEntry = await storage.getJournalEntry(entry.id);
            if (completeEntry) {
              const syncSuccess = await syncJournalEntryToNotion(completeEntry);
              if (syncSuccess) {
                console.log(`Entry ${entry.id} synced to Notion successfully`);
              }
            }
          } catch (notionError) {
            console.error(`Notion sync failed for entry ${entry.id}:`, notionError);
          }

          console.log(`Entry ${entry.id} completed successfully`);
        }
      } else {
        console.log(`No text extracted from entry ${entry.id}, marking as failed`);
        await storage.updateJournalEntry(entry.id, {
          processingStatus: "failed",
          transcribedText: "No readable text found in image"
        });
      }
    } catch (processingError) {
      console.error(`Processing error for entry ${entry.id}:`, processingError);
      const errorMessage = processingError instanceof Error ? processingError.message : "Unknown processing error";
      await storage.updateJournalEntry(entry.id, {
        processingStatus: "failed",
        transcribedText: `Processing failed: ${errorMessage}`
      });
    }
  }
  
  // Clean up batch tracking when complete
  activeBatches.delete(batchId);
  console.log(`Bulk processing completed for user ${userId}, batch ${batchId}`);
}

// Configure multer for file uploads
const upload = multer({
  storage: multer.diskStorage({
    destination: async (req, file, cb) => {
      const uploadDir = path.join(process.cwd(), 'uploads');
      try {
        await fs.mkdir(uploadDir, { recursive: true });
        cb(null, uploadDir);
      } catch (error) {
        cb(error as any, uploadDir);
      }
    },
    filename: (req, file, cb) => {
      const uniqueSuffix = Date.now() + '-' + Math.round(Math.random() * 1E9);
      cb(null, file.fieldname + '-' + uniqueSuffix + path.extname(file.originalname));
    }
  }),
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB limit
  },
  fileFilter: (req, file, cb) => {
    const allowedTypes = ['image/jpeg', 'image/png', 'image/gif', 'image/webp', 'image/heic', 'image/heif'];
    if (allowedTypes.includes(file.mimetype)) {
      cb(null, true);
    } else {
      cb(new Error('Only image files are allowed') as any, false);
    }
  }
});

// Authentication middleware
function requireAuth(req: Request, res: Response, next: NextFunction) {
  console.log("Auth check:", {
    hasSession: !!req.session,
    userId: req.session?.userId,
    sessionId: req.session?.id,
    path: req.path
  });
  
  if (!req.session?.userId) {
    console.log("Authentication failed - no session or userId");
    return res.status(401).json({ 
      message: "Authentication required",
      hasSession: !!req.session,
      sessionExists: req.session?.id ? true : false
    });
  }
  next();
}

export async function registerRoutes(app: Express): Promise<Server> {
  // Authentication routes
  app.post("/api/auth/login", async (req, res) => {
    try {
      const { username, password } = req.body;
      
      if (!username || !password) {
        return res.status(400).json({ message: "Username and password are required" });
      }

      const result = await authenticateUser(username, password);
      
      if (result.success && result.user) {
        req.session.userId = result.user.id;
        req.session.user = result.user;
        return res.json({ success: true, user: result.user });
      } else {
        return res.status(401).json({ success: false, message: result.error || "Invalid credentials" });
      }
    } catch (error) {
      console.error("Login error:", error);
      return res.status(500).json({ success: false, message: "Login failed" });
    }
  });

  app.post("/api/auth/register", async (req, res) => {
    try {
      const { username, password } = req.body;
      
      if (!username || !password) {
        return res.status(400).json({ message: "Username and password are required" });
      }

      const usernameValidation = validateUsername(username);
      if (!usernameValidation.valid) {
        return res.status(400).json({ message: usernameValidation.error });
      }

      const passwordValidation = validatePassword(password);
      if (!passwordValidation.valid) {
        return res.status(400).json({ message: passwordValidation.error });
      }

      const result = await createUser(username, password);
      
      if (result.success && result.user) {
        req.session.userId = result.user.id;
        req.session.user = result.user;
        return res.json({ success: true, user: result.user });
      } else {
        return res.status(400).json({ success: false, message: result.error || "Registration failed" });
      }
    } catch (error) {
      console.error("Registration error:", error);
      return res.status(500).json({ success: false, message: "Registration failed" });
    }
  });

  app.post("/api/auth/logout", (req, res) => {
    req.session.destroy((err) => {
      if (err) {
        return res.status(500).json({ message: "Logout failed" });
      }
      res.clearCookie('connect.sid');
      res.json({ message: "Logged out successfully" });
    });
  });

  app.get("/api/auth/me", (req, res) => {
    if (req.session?.userId && req.session?.user) {
      res.json({ user: req.session.user });
    } else {
      res.status(401).json({ message: "Not authenticated" });
    }
  });

  // Journal entry routes
  app.post("/api/journal-entries", requireAuth, async (req, res) => {
    try {
      const { title, transcribedText, processingStatus = "completed" } = req.body;
      
      if (!title || !transcribedText) {
        return res.status(400).json({ message: "Title and transcribed text are required" });
      }

      const entry = await storage.createJournalEntry({
        userId: req.session.userId!,
        title,
        transcribedText,
        processingStatus,
        originalImageUrl: null,
        ocrConfidence: null,
        imageHash: null,
        transcriptHash: null
      });

      res.json(entry);
    } catch (error) {
      console.error("Create entry error:", error);
      res.status(500).json({ message: "Failed to create journal entry" });
    }
  });

  app.get("/api/journal-entries", requireAuth, async (req, res) => {
    try {
      const limit = parseInt(req.query.limit as string) || 10;
      const entries = await storage.getJournalEntriesByUser(req.session.userId!, limit);
      res.json(entries);
    } catch (error) {
      console.error("Get entries error:", error);
      res.status(500).json({ message: "Failed to fetch journal entries" });
    }
  });

  app.get("/api/journal-entries/:id", requireAuth, async (req, res) => {
    try {
      const id = parseInt(req.params.id);
      const entry = await storage.getJournalEntry(id, req.session.userId!);
      
      if (!entry) {
        return res.status(404).json({ message: "Entry not found" });
      }
      
      res.json(entry);
    } catch (error) {
      console.error("Get entry error:", error);
      res.status(500).json({ message: "Failed to fetch journal entry" });
    }
  });

  app.get("/api/journal-entries/:id/status", requireAuth, async (req, res) => {
    try {
      const id = parseInt(req.params.id);
      
      const entry = await storage.getJournalEntry(id, req.session.userId!);
      if (!entry) {
        return res.status(404).json({ message: "Entry not found" });
      }

      res.json({
        id: entry.id,
        processingStatus: entry.processingStatus,
        transcribedText: entry.transcribedText,
        ocrConfidence: entry.ocrConfidence,
        title: entry.title,
        updatedAt: entry.updatedAt
      });
    } catch (error) {
      console.error("Status check error:", error);
      res.status(500).json({ message: "Failed to check status" });
    }
  });

  app.patch("/api/journal-entries/:id", requireAuth, async (req, res) => {
    console.log(`PATCH request received for entry ${req.params.id}`);
    console.log(`Request body:`, req.body);
    console.log(`Content-Type:`, req.headers['content-type']);
    
    try {
      const id = parseInt(req.params.id);
      const { title, transcribedText } = req.body;
      
      if (!transcribedText || typeof transcribedText !== 'string') {
        console.log(`Validation failed: transcribedText missing or invalid`);
        return res.status(400).json({ message: "Transcribed text is required" });
      }
      
      const entry = await storage.getJournalEntry(id, req.session.userId!);
      if (!entry) {
        console.log(`Entry not found for entry ${id}`);
        return res.status(404).json({ message: "Entry not found" });
      }

      console.log(`Updating entry ${id} with new text of length ${transcribedText.length}`);
      const updatedEntry = await storage.updateJournalEntry(id, {
        title,
        transcribedText: transcribedText.trim()
      });

      // Sync to Notion if integration is set up
      try {
        const updatedEntryWithDetails = await storage.getJournalEntry(id, req.session.userId!);
        if (updatedEntryWithDetails) {
          const syncSuccess = await syncJournalEntryToNotion(updatedEntryWithDetails);
          if (syncSuccess) {
            console.log(`Entry ${id} synced to Notion successfully`);
          }
        }
      } catch (notionError) {
        console.error(`Notion sync failed for entry ${id}:`, notionError);
        // Don't fail the request if Notion sync fails
      }

      console.log(`Entry ${id} updated successfully`);
      res.json({
        message: "Entry updated successfully",
        entry: updatedEntry
      });
    } catch (error) {
      console.error("Edit entry error:", error);
      res.status(500).json({ message: "Failed to edit journal entry" });
    }
  });

  app.post("/api/journal-entries/:id/edit", requireAuth, async (req, res) => {
    try {
      const id = parseInt(req.params.id);
      const { title, transcribedText } = req.body;
      
      const entry = await storage.getJournalEntry(id, req.session.userId!);
      if (!entry) {
        return res.status(404).json({ message: "Entry not found" });
      }

      const updatedEntry = await storage.updateJournalEntry(id, {
        title,
        transcribedText
      });

      res.json(updatedEntry);
    } catch (error) {
      console.error("Edit entry error:", error);
      res.status(500).json({ message: "Failed to edit journal entry" });
    }
  });

  // Bulk delete entries endpoint (must come before single delete to avoid route conflicts)
  app.delete("/api/journal-entries/bulk", requireAuth, async (req, res) => {
    try {
      console.log("=== BULK DELETE ENDPOINT HIT ===");
      console.log("Bulk delete request body:", JSON.stringify(req.body));
      console.log("Request method:", req.method);
      console.log("Request path:", req.path);
      const { entryIds } = req.body;
      
      if (!Array.isArray(entryIds) || entryIds.length === 0) {
        console.log("Invalid entryIds array:", entryIds);
        return res.status(400).json({ message: "Entry IDs array is required" });
      }

      console.log("Processing bulk delete for IDs:", entryIds);
      const results = [];
      const errors = [];

      // Pre-validate all IDs before any database operations
      const validIds = [];
      for (let i = 0; i < entryIds.length; i++) {
        const entryId = entryIds[i];
        console.log(`Processing entryId[${i}]: ${entryId} (type: ${typeof entryId})`);
        
        // Handle both string and number inputs
        let parsedId;
        if (typeof entryId === 'number') {
          parsedId = entryId;
        } else if (typeof entryId === 'string') {
          parsedId = parseInt(entryId, 10);
        } else {
          console.log(`Invalid entry ID type: ${entryId} (type: ${typeof entryId})`);
          errors.push({ entryId, error: "Invalid entry ID type" });
          continue;
        }
        
        if (!Number.isInteger(parsedId) || isNaN(parsedId) || parsedId <= 0) {
          console.log(`Invalid entry ID detected: ${entryId} (parsed: ${parsedId})`);
          errors.push({ entryId, error: "Invalid entry ID format" });
          continue;
        }
        validIds.push(parsedId);
      }

      console.log("Valid IDs to process:", validIds);

      for (const entryId of validIds) {
        try {
          // Verify the entry belongs to the user
          const entry = await storage.getJournalEntry(entryId, req.session.userId!);
          if (!entry) {
            errors.push({ entryId, error: "Entry not found or unauthorized" });
            continue;
          }

          await storage.deleteJournalEntry(entryId);
          results.push({ entryId, status: "deleted" });
        } catch (error) {
          console.error(`Failed to delete entry ${entryId}:`, error);
          errors.push({ entryId, error: error instanceof Error ? error.message : "Unknown error" });
        }
      }

      res.json({
        success: true,
        deletedCount: results.length,
        requestedCount: entryIds.length,
        errors: errors.length > 0 ? errors : undefined
      });
    } catch (error) {
      console.error("Bulk delete error:", error);
      res.status(500).json({ message: "Failed to delete entries" });
    }
  });

  app.delete("/api/journal-entries/:id", requireAuth, async (req, res) => {
    try {
      console.log("Single delete request for ID:", req.params.id);
      const id = parseInt(req.params.id);
      
      if (isNaN(id) || id <= 0) {
        console.log("Invalid ID in single delete:", req.params.id, "parsed to:", id);
        return res.status(400).json({ message: "Invalid entry ID" });
      }
      
      const entry = await storage.getJournalEntry(id, req.session.userId!);
      if (!entry) {
        return res.status(404).json({ message: "Entry not found" });
      }

      await storage.deleteJournalEntry(id);
      res.json({ message: "Entry deleted successfully" });
    } catch (error) {
      console.error("Delete entry error:", error);
      res.status(500).json({ message: "Failed to delete journal entry" });
    }
  });

  // Merge journal entries endpoint
  app.post("/api/journal-entries/merge", requireAuth, async (req, res) => {
    try {
      const { entryIds, mergedTitle, regenerateAnalysis = false, deleteOriginals = true } = req.body;
      
      if (!Array.isArray(entryIds) || entryIds.length < 2) {
        return res.status(400).json({ message: "At least 2 entry IDs are required for merging" });
      }

      if (!mergedTitle || mergedTitle.trim().length === 0) {
        return res.status(400).json({ message: "Merged title is required" });
      }

      console.log(`Merging entries ${entryIds.join(', ')} for user ${req.session.userId}`);
      
      const mergedEntry = await storage.mergeJournalEntries(
        req.session.userId!,
        entryIds,
        mergedTitle.trim(),
        regenerateAnalysis
      );

      // Get the merged entry with full details for Notion sync
      const mergedEntryWithDetails = await storage.getJournalEntry(mergedEntry.id);
      
      // Handle Notion cleanup and sync for merged entries
      try {
        await cleanupNotionEntriesForMerge(entryIds, mergedEntryWithDetails!);
        console.log(`Notion cleanup completed for merged entry ${mergedEntry.id}`);
      } catch (error) {
        console.error(`Notion cleanup failed for merged entry ${mergedEntry.id}:`, error);
        // Continue with merge operation even if Notion sync fails
      }

      // Optionally delete original entries
      if (deleteOriginals) {
        for (const entryId of entryIds) {
          try {
            await storage.deleteJournalEntry(entryId);
            console.log(`Deleted original entry ${entryId}`);
          } catch (error) {
            console.error(`Failed to delete original entry ${entryId}:`, error);
          }
        }
      }

      res.json({
        success: true,
        mergedEntry: mergedEntryWithDetails,
        deletedOriginals: deleteOriginals
      });
    } catch (error) {
      console.error("Merge entries error:", error);
      res.status(500).json({ 
        message: error instanceof Error ? error.message : "Failed to merge journal entries" 
      });
    }
  });

  app.post("/api/upload", requireAuth, upload.single('journal'), async (req, res) => {
    let entry;
    try {
      if (!req.file) {
        return res.status(400).json({ message: "No file uploaded" });
      }

      const userId = req.session.userId!;
      const { title } = req.body;

      // Convert absolute file path to web URL
      const filename = path.basename(req.file.path);
      const webImageUrl = `/uploads/${filename}`;

      // Check for duplicate image
      let imageHash;
      try {
        const { computeImageHash } = await import('./image-hash');
        imageHash = await computeImageHash(req.file.path);
        
        const existingEntry = await storage.getJournalEntryByImageHash(imageHash);
        if (existingEntry) {
          return res.status(400).json({ 
            message: "Duplicate image detected",
            duplicate: true,
            existingEntryId: existingEntry.id
          });
        }

        entry = await storage.createJournalEntry({
          userId,
          title: title || "Untitled Entry",
          originalImageUrl: webImageUrl,
          transcribedText: "",
          ocrConfidence: 0,
          processingStatus: "processing",
          imageHash
        });

        res.json(entry);
      } catch (hashError) {
        console.error(`Error computing hash for ${req.file.originalname}:`, hashError);
        // If hashing fails, proceed without hash
        entry = await storage.createJournalEntry({
          userId,
          title: title || "Untitled Entry",
          originalImageUrl: webImageUrl,
          transcribedText: "",
          ocrConfidence: 0,
          processingStatus: "processing"
        });

        res.json(entry);
      }

      // Process asynchronously
      try {
        console.log(`Starting OCR processing for entry ${entry.id}`);
        const ocrResult = await extractTextFromImage(req.file.path);
        console.log(`OCR completed for entry ${entry.id}, text length: ${ocrResult.text?.length || 0}`);
        
        if (ocrResult.text && ocrResult.text.trim().length > 0) {
          console.log(`OCR completed for entry ${entry.id}, checking for transcript duplicates...`);
          
          // Check for transcript duplicates
          const { computeTranscriptHash } = await import('./image-hash');
          const transcriptHash = computeTranscriptHash(ocrResult.text);
          
          const existingTranscriptEntry = await storage.getJournalEntryByTranscriptHash(transcriptHash);
          if (existingTranscriptEntry) {
            console.log(`Duplicate transcript detected for entry ${entry.id} (matches entry ${existingTranscriptEntry.id}), marking as failed`);
            await storage.updateJournalEntry(entry.id, {
              processingStatus: "failed",
              transcribedText: `Duplicate content detected (matches entry ${existingTranscriptEntry.id})`
            });
            return;
          }
          
          console.log(`No transcript duplicates found for entry ${entry.id}, checking for multiple dates...`);
          
          // Check for multiple dates and split if needed
          const splitEntries = splitEntriesByDate(ocrResult.text);
          console.log(`Found ${splitEntries.length} date-based entries in OCR text`);
          
          if (splitEntries.length > 1) {
            // Multiple entries detected - process each separately
            console.log(`Splitting entry ${entry.id} into ${splitEntries.length} separate entries`);
            
            // Update the original entry with the first split
            const firstEntry = splitEntries[0];
            console.log(`Processing first split entry with date: ${formatDateForLog(firstEntry.date)}`);
            
            const firstAnalysis = await analyzeJournalEntry(firstEntry.content);
            const firstSentiment = await analyzeSentiment(firstEntry.content);
            
            await storage.updateJournalEntry(entry.id, {
              title: firstAnalysis.title || `Journal Entry - ${formatDateForLog(firstEntry.date)}`,
              transcribedText: firstEntry.content,
              ocrConfidence: ocrResult.confidence,
              processingStatus: "completed",
              transcriptHash
            } as any);
            
            // Process themes, tags, and sentiment for first entry
            await processEntryAnalysis(entry.id, firstAnalysis, firstSentiment);
            
            // Create new entries for remaining splits
            for (let j = 1; j < splitEntries.length; j++) {
              const splitEntry = splitEntries[j];
              console.log(`Creating new entry for split ${j + 1} with date: ${formatDateForLog(splitEntry.date)}`);
              
              // Compute hash for each split content
              const splitTranscriptHash = computeTranscriptHash(splitEntry.content);
              
              const newEntry = await storage.createJournalEntry({
                userId: entry.userId,
                title: `Journal Entry - ${formatDateForLog(splitEntry.date)}`,
                originalImageUrl: entry.originalImageUrl, // Reuse same image
                transcribedText: splitEntry.content,
                ocrConfidence: ocrResult.confidence,
                processingStatus: "completed",
                transcriptHash: splitTranscriptHash
              } as any);
              
              const splitAnalysis = await analyzeJournalEntry(splitEntry.content);
              const splitSentiment = await analyzeSentiment(splitEntry.content);
              
              // Update title with AI-generated one
              await storage.updateJournalEntry(newEntry.id, {
                title: splitAnalysis.title || newEntry.title
              });
              
              // Process themes, tags, and sentiment for split entry
              await processEntryAnalysis(newEntry.id, splitAnalysis, splitSentiment);
              
              // Sync split entry to Notion
              try {
                const completeNewEntry = await storage.getJournalEntry(newEntry.id);
                if (completeNewEntry) {
                  const syncSuccess = await syncJournalEntryToNotion(completeNewEntry);
                  if (syncSuccess) {
                    console.log(`Split entry ${newEntry.id} synced to Notion successfully`);
                  }
                }
              } catch (notionError) {
                console.error(`Notion sync failed for split entry ${newEntry.id}:`, notionError);
              }
            }
            
            console.log(`Entry ${entry.id} split into ${splitEntries.length} entries successfully`);
          } else {
            // Single entry - process normally
            console.log(`Processing single entry ${entry.id}`);
            const singleEntry = splitEntries[0];
            
            const analysisResult = await analyzeJournalEntry(singleEntry.content);
            const sentimentResult = await analyzeSentiment(singleEntry.content);

            await storage.updateJournalEntry(entry.id, {
              title: analysisResult.title || entry.title,
              transcribedText: singleEntry.content,
              ocrConfidence: ocrResult.confidence,
              processingStatus: "completed",
              transcriptHash
            } as any);

            // Process themes, tags, and sentiment
            await processEntryAnalysis(entry.id, analysisResult, sentimentResult);
          }

          const completeEntry = await storage.getJournalEntry(entry.id);
          
          // Sync to Notion if enabled
          const notionIntegration = await storage.getUserIntegration(userId, "notion");
          if (notionIntegration?.isEnabled && completeEntry) {
            try {
              const syncSuccess = await syncJournalEntryToNotion(completeEntry);
              if (syncSuccess) {
                console.log(`Entry ${entry.id} synced to Notion successfully`);
              }
            } catch (notionError) {
              console.error("Notion sync failed:", notionError);
            }
          }
        } else {
          console.log(`No text extracted from entry ${entry.id}, marking as failed`);
          await storage.updateJournalEntry(entry.id, {
            processingStatus: "failed",
            transcribedText: "No readable text found in image"
          });
        }
      } catch (processingError) {
        console.error(`Processing error for entry ${entry.id}:`, processingError);
        const errorMessage = processingError instanceof Error ? processingError.message : "Unknown processing error";
        await storage.updateJournalEntry(entry.id, {
          processingStatus: "failed",
          transcribedText: `Processing failed: ${errorMessage}`
        });
      }
    } catch (error) {
      console.error("Upload error:", error);
      res.status(500).json({ message: "Upload failed" });
    }
  });

  // Bulk upload with improved progress tracking (up to 100 files)
  app.post("/api/upload-bulk", requireAuth, upload.array('journals', 100), async (req, res) => {
    try {
      if (!req.files || !Array.isArray(req.files) || req.files.length === 0) {
        return res.status(400).json({ message: "No files uploaded" });
      }

      if (req.files.length > 100) {
        return res.status(400).json({ message: "Maximum 100 files allowed per batch" });
      }

      const userId = req.session.userId!;
      const batchId = `bulk_${Date.now()}_${userId}`;
      const entries = [];
      const skippedDuplicates: string[] = [];

      console.log(`Starting bulk upload of ${req.files.length} files for user ${userId}`);

      // First pass: Check for duplicate images and create entries for unique files
      for (let i = 0; i < req.files.length; i++) {
        const file = req.files[i] as Express.Multer.File;
        const filename = path.basename(file.path);
        const webImageUrl = `/uploads/${filename}`;
        
        try {
          // Compute image hash for duplicate detection
          const { computeImageHash } = await import('./image-hash');
          const imageHash = await computeImageHash(file.path);
          
          // Check if image hash already exists
          const existingEntry = await storage.getJournalEntryByImageHash(imageHash);
          if (existingEntry) {
            console.log(`Duplicate image detected: ${file.originalname} (matches entry ${existingEntry.id})`);
            skippedDuplicates.push(file.originalname);
            continue;
          }
          
          // Create entry for unique image
          const entry = await storage.createJournalEntry({
            userId,
            title: file.originalname.replace(/\.[^/.]+$/, "") || `Upload ${i + 1}`,
            originalImageUrl: webImageUrl,
            transcribedText: "",
            ocrConfidence: 0,
            processingStatus: "processing",
            imageHash
          });
          entries.push({ ...entry, filePath: file.path });
        } catch (hashError) {
          console.error(`Error computing hash for ${file.originalname}:`, hashError);
          // If hashing fails, proceed without hash (may create duplicates)
          const entry = await storage.createJournalEntry({
            userId,
            title: file.originalname.replace(/\.[^/.]+$/, "") || `Upload ${i + 1}`,
            originalImageUrl: webImageUrl,
            transcribedText: "",
            ocrConfidence: 0,
            processingStatus: "processing"
          });
          entries.push({ ...entry, filePath: file.path });
        }
      }

      // Store batch info for progress tracking
      const entryIds = entries.map(e => e.id);
      activeBatches.set(batchId, entryIds);

      // Respond immediately with entry IDs for progress tracking
      res.json({ 
        processed: entries.map(e => ({ ...e, filePath: undefined })),
        skipped_duplicates: skippedDuplicates,
        batchId,
        message: `${entries.length} files uploaded${skippedDuplicates.length > 0 ? `, ${skippedDuplicates.length} duplicates skipped` : ''}, processing sequentially...` 
      });

      // Process files sequentially in background
      processBulkEntriesSequentially(entries, userId, batchId);

    } catch (error) {
      console.error("Bulk upload error:", error);
      res.status(500).json({ message: "Bulk upload failed" });
    }
  });

  // Progress tracking endpoint
  app.get("/api/bulk-progress/:batchId", requireAuth, async (req, res) => {
    try {
      const { batchId } = req.params;
      const userId = req.session.userId!;
      
      // Get the specific entry IDs for this batch
      const batchEntryIds = activeBatches.get(batchId);
      
      if (!batchEntryIds) {
        // Batch not found or completed, return completed status
        return res.json({
          total: 0,
          completed: 0,
          failed: 0,
          processing: 0,
          progress: 100
        });
      }
      
      // Get the current status of each entry in this batch
      let completed = 0;
      let failed = 0;
      let processing = 0;
      
      for (const entryId of batchEntryIds) {
        try {
          const entry = await storage.getJournalEntry(entryId);
          if (entry && entry.userId === userId) {
            switch (entry.processingStatus) {
              case 'completed':
                completed++;
                break;
              case 'failed':
                failed++;
                break;
              case 'processing':
              default:
                processing++;
                break;
            }
          }
        } catch (entryError) {
          console.error(`Error checking entry ${entryId}:`, entryError);
          failed++;
        }
      }
      
      const total = batchEntryIds.length;
      const progress = total > 0 ? Math.round((completed + failed) / total * 100) : 100;
      
      res.json({
        total,
        completed,
        failed,
        processing,
        progress
      });
    } catch (error) {
      console.error("Progress tracking error:", error);
      res.status(500).json({ message: "Failed to get progress" });
    }
  });

  // Notion integration routes
  app.get("/api/integrations/notion", requireAuth, async (req, res) => {
    try {
      const integration = await storage.getUserIntegration(req.session.userId!, "notion");
      
      if (!integration) {
        return res.json({ enabled: false, configured: false });
      }

      res.json({
        enabled: integration.isEnabled,
        configured: !!integration.config,
        config: integration.config
      });
    } catch (error) {
      console.error("Get Notion integration error:", error);
      res.status(500).json({ message: "Failed to get Notion integration status" });
    }
  });

  app.post("/api/integrations/notion/configure", requireAuth, async (req, res) => {
    try {
      const { integrationToken, pageUrl } = req.body;
      
      if (!integrationToken || !pageUrl) {
        return res.status(400).json({ message: "Integration token and page URL are required" });
      }

      const notion = createNotionClient(integrationToken);
      const pageId = extractPageIdFromUrl(pageUrl);
      
      await createJournalDatabase(notion, pageId);

      const config = {
        integrationToken,
        pageUrl,
        pageId
      };

      const existingIntegration = await storage.getUserIntegration(req.session.userId!, "notion");
      
      if (existingIntegration) {
        await storage.updateUserIntegration(req.session.userId!, "notion", {
          config,
          isEnabled: true
        });
      } else {
        await storage.createUserIntegration({
          userId: req.session.userId!,
          integrationType: "notion",
          isEnabled: true,
          config
        });
      }

      res.json({ success: true, message: "Notion integration configured successfully" });
    } catch (error) {
      console.error("Configure Notion error:", error);
      res.status(500).json({ message: "Failed to configure Notion integration" });
    }
  });

  app.post("/api/integrations/notion/toggle", requireAuth, async (req, res) => {
    try {
      const { enabled } = req.body;
      
      const integration = await storage.getUserIntegration(req.session.userId!, "notion");
      
      if (!integration) {
        return res.status(404).json({ message: "Notion integration not found" });
      }

      await storage.updateUserIntegration(req.session.userId!, "notion", {
        isEnabled: enabled
      });

      res.json({
        enabled,
        message: enabled ? "Notion integration enabled" : "Notion integration disabled"
      });
    } catch (error) {
      console.error("Toggle Notion integration error:", error);
      res.status(500).json({ message: "Failed to toggle Notion integration" });
    }
  });

  app.post("/api/integrations/notion/sync-all", requireAuth, async (req, res) => {
    try {
      const integration = await storage.getUserIntegration(req.session.userId!, "notion");
      
      if (!integration || !integration.isEnabled) {
        return res.status(400).json({ message: "Notion integration not enabled" });
      }

      const result = await syncAllUserEntriesToNotion(req.session.userId!);
      
      res.json({ 
        message: `Sync completed: ${result.success} successful, ${result.failed} failed`,
        successCount: result.success,
        failedCount: result.failed
      });
    } catch (error) {
      console.error("Bulk sync error:", error);
      res.status(500).json({ 
        message: "Bulk sync failed. Please try again." 
      });
    }
  });

  // Retry failed entries
  app.post("/api/retry-failed", requireAuth, async (req, res) => {
    try {
      const result = await retryFailedEntries();
      res.json(result);
    } catch (error) {
      console.error("Retry failed entries error:", error);
      res.status(500).json({ message: "Failed to retry processing" });
    }
  });

  app.get("/api/failed-entries-count", requireAuth, async (req, res) => {
    try {
      const failedEntries = await storage.getFailedEntries();
      res.json({ count: failedEntries.length });
    } catch (error) {
      console.error("Get failed entries count error:", error);
      res.status(500).json({ message: "Failed to get failed entries count" });
    }
  });

  // Tags routes
  app.get("/api/tags", requireAuth, async (req, res) => {
    try {
      const tags = await storage.getAllTags();
      res.json(tags);
    } catch (error) {
      console.error("Get tags error:", error);
      res.status(500).json({ message: "Failed to fetch tags" });
    }
  });





  // Health check endpoint
  app.get("/api/health", (req, res) => {
    res.json({ 
      status: "ok", 
      timestamp: new Date().toISOString()
    });
  });

  const httpServer = createServer(app);
  return httpServer;
}
